Project Sentinel: Progress Report & Procedure BookDate: September 15, 2025Version: 1.0Part 1: Progress Report1.1 Current Status: Foundation LaidWe have successfully completed the foundational architecture and database setup phase. The core infrastructure is defined, the data models are specified, and the initial services for ingestion and crawling have been implemented.Key Accomplishments:Comprehensive Docker Stack Defined: A robust docker-compose.yml has been created, orchestrating over a dozen services covering API management, data persistence, AI inference, and observability.Database Schema Established: A detailed and normalized PostgreSQL schema is complete, featuring tables, views, indexes, and functions. Vector capabilities with pgvector are integrated.Pydantic Models Created: All data structures are strictly defined in schemas.py, ensuring type safety and clear data contracts across all Python-based microservices.Initial Services Deployed: The core agentic-kg API service and the crawler service are containerized and integrated into the stack.1.2 Architectural OverviewOur system is a decoupled, microservices-based platform designed for scalability and maintainability.High-Level System Diagram:This diagram illustrates how user requests flow through the Kong API Gateway to various backend services, which in turn communicate with the data persistence layer and the AI inference engines. n8n acts as the central orchestrator for automated data pipelines.Data Ingestion Workflow:This flowchart shows the step-by-step process: The Crawler fetches a document, the text is sent to the API, where LlamaIndex and LangGraph agents process it, extract entities, generate embeddings, and store the results in PostgreSQL, Neo4j, and Qdrant.1.3 What's Next: The Path to Version 1.0The next steps focus on activating the architecture we've built:Activate Core Infrastructure: Add MinIO for object storage and configure Kong.Define Data Targets: Finalize the comprehensive list of sources for the crawler.Develop Core AI Agents: Begin coding the agent logic using LangGraph.Build the User Interface: Create the Next.js frontend to provide a control panel and data visualization interface for the entire system.Part 2: Procedure Book & Methodology2.1 Core Investigative MethodologyThe platform's primary goal is to generate objective, data-driven profiles of political entities (politicians, agencies, governments) based on public data. Our analysis will be centered around a set of key metrics.Key Metrics & Definitions:Truthfulness & Consistency: Comparing a politician's public statements (from social media, press releases) against their voting record and sponsored legislation. Discrepancies are flagged.Flip-Flop Index: Tracking changes in voting patterns or stated positions on key topics over time.Rhetoric Analysis: Using NLP models to scan for inflammatory language, hate speech, or excessive repetition of emotionally charged phrases.Voting Record Analysis: Moving beyond simple "Yea/Nay" to analyze what a vote was for. This involves linking votes to specific bills and bill topics.Political Bias: Analyzing the language used in official reports and documents to identify potential bias, using models trained to detect slanted or leading language.2.2 Analysis LevelsThe methodology will be applied across multiple jurisdictional levels to provide a comprehensive picture:Federal Level: Analyzing members of Congress, federal agencies (CIA, FBI, etc.), and presidential administrations.State Level: Analyzing state legislators, governors, and state-level agencies.Local Level: (Future Phase) Analyzing city councils and municipal governments.2.3 Configuration ManagementTo ensure security and maintainability, all sensitive information will be managed through environment variables and stored securely.API Keys & Passwords: Will be injected into services via the docker-compose.yml and a local .env file. They will never be hardcoded.Crawler & Ingestion Configurations: The list of target sources and specific ingestion methodologies will be stored in a dedicated table within the PostgreSQL database. This allows the configuration to be updated dynamically via the application's UI without requiring a code deployment.This document will serve as the guiding blueprint for the next phase of development.
